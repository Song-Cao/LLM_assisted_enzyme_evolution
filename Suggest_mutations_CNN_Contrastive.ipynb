{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1","authorship_tag":"ABX9TyO0fAbTx2y2ewNOyeo1EW2j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeiORzTV0W01","executionInfo":{"status":"ok","timestamp":1749540334640,"user_tz":-60,"elapsed":16499,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}},"outputId":"5bb6edb1-07f6-46a8-dbc8-a775a32666b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import math\n","import random\n","import itertools\n","import re\n","import torch.optim as optim\n","from scipy.stats import spearmanr\n","from transformers import AutoTokenizer, AutoModel"],"metadata":{"id":"NZ8WrYeC0dXT","executionInfo":{"status":"ok","timestamp":1749540348783,"user_tz":-60,"elapsed":12234,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class BasicBlock1D(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, downsample=None):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes,\n","                               kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1   = nn.BatchNorm1d(planes)\n","        self.relu  = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv1d(planes, planes,\n","                               kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","        self.bn2   = nn.BatchNorm1d(planes)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(identity)\n","\n","        out += identity\n","        return self.relu(out)\n","\n","\n","class CNN_Contrastive(nn.Module):\n","    def __init__(self, d_esm: int, use_first_fitness: bool = False):\n","        \"\"\"\n","        CNN‐based contrastive model using a 1D-ResNet18 backbone.\n","\n","        Args:\n","        -----\n","        d_esm: int\n","            Dimension of the input ESM embeddings.\n","        use_first_fitness: bool\n","            If True, append the fitness value of the first variant before the final FC.\n","        \"\"\"\n","        super().__init__()\n","        self.use_first_fitness = use_first_fitness\n","        self.inplanes = 64\n","\n","        # --- initial conv/bn/relu/pool (1D) ---\n","        self.conv1   = nn.Conv1d(d_esm, 64,\n","                                 kernel_size=7, stride=2,\n","                                 padding=3, bias=False)\n","        self.bn1     = nn.BatchNorm1d(64)\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool1d(kernel_size=3,\n","                                    stride=2, padding=1)\n","\n","        # --- the 4 ResNet layers (2 blocks each) ---\n","        self.layer1 = self._make_layer(BasicBlock1D,  64, 2)\n","        self.layer2 = self._make_layer(BasicBlock1D, 128, 2, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock1D, 256, 2, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock1D, 512, 2, stride=2)\n","\n","        # --- global pooling + final FC ---\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        fc_in = 512 * BasicBlock1D.expansion + (1 if use_first_fitness else 0)\n","        self.fc = nn.Linear(fc_in, 1)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv1d(self.inplanes,\n","                          planes * block.expansion,\n","                          kernel_size=1,\n","                          stride=stride,\n","                          bias=False),\n","                nn.BatchNorm1d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes,\n","                            stride=stride,\n","                            downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, emb1, emb2, fit1= None):\n","        \"\"\"\n","        Args:\n","        -----\n","        emb1, emb2: (N, L, d_esm)\n","            ESM embeddings for sequence 1 and 2.\n","        fit1: (N, 1), optional\n","            The 1‐D fitness feature of the first variant to append before the final FC.\n","\n","        Returns:\n","        --------\n","        out: (N,)\n","            The scalar logit/regression output.\n","        \"\"\"\n","        # compute difference and permute to (N, d_esm, L)\n","        x = emb2 - emb1\n","        x = x.permute(0, 2, 1)\n","\n","        # stem\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        # ResNet layers\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # global pooling\n","        x = self.avgpool(x)     # (N, 512, 1)\n","        x = torch.flatten(x, 1) # (N, 512)\n","\n","        # 5) optionally append Fitness 1\n","        if self.use_first_fitness and fit1 is not None:\n","            x = torch.cat([x, fit1], dim=1)  # (N, 513)\n","\n","        # 6) final FC -> scalar\n","        out = self.fc(x)  # (N, 1)\n","        return out.squeeze(1)  # (N,)"],"metadata":{"id":"nT7hpWsG0lYA","executionInfo":{"status":"ok","timestamp":1749540373348,"user_tz":-60,"elapsed":57,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def load_data(idx1, idx2, fit1, labels, emb, batch, batch_size, device):\n","    \"\"\"\n","    Returns:\n","      emb1_batch: (B, L, d_esm) torch.Tensor\n","      emb2_batch: (B, L, d_esm) torch.Tensor\n","      fit1_batch: (B, 1)      torch.Tensor\n","      labels_batch:(B,)       torch.Tensor\n","    \"\"\"\n","    start = batch * batch_size\n","    end   = min(start + batch_size, len(labels))\n","    b_idx1 = idx1[start:end]\n","    b_idx2 = idx2[start:end]\n","    b_fit1 = np.array(fit1[start:end], dtype=np.float32).reshape(-1,1)\n","    b_lbl  = np.array(labels[start:end], dtype=np.float32).reshape(-1,1)\n","\n","    emb1 = emb[np.array(b_idx1)]\n","    emb2 = emb[np.array(b_idx2)]\n","    emb1 = torch.from_numpy(emb1).float().to(device)\n","    emb2 = torch.from_numpy(emb2).float().to(device)\n","    fit1 = torch.from_numpy(b_fit1).float().to(device)\n","    lbl  = torch.from_numpy(b_lbl).squeeze(1).float().to(device)\n","    return emb1, emb2, fit1, lbl\n","\n","def train_epoch(model, optimizer, idx1, idx2, fit1, labels, emb, batch_size, epoch, device, train_frac):\n","    model.train()\n","    criterion = nn.MSELoss()\n","    running_loss = 0.0\n","    total = 0\n","    num_batches = math.ceil(len(labels) / batch_size)\n","    batch_set_size = math.ceil(num_batches/train_frac)\n","    batch_set_idx = (epoch-1)%train_frac\n","    start_b = batch_set_idx * batch_set_size\n","    end_b = (batch_set_idx+1) * batch_set_size\n","    for b in tqdm(list(range(num_batches))[start_b: end_b], desc=f\"Epoch {epoch}\"):\n","        e1, e2, f1, y = load_data(idx1, idx2, fit1, labels, emb, b, batch_size, device)\n","        preds = model(e1, e2, f1)\n","        loss = criterion(preds, y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * preds.size(0)\n","        total += preds.size(0)\n","    return running_loss / total\n","\n","\n","def test_epoch(model, idx1, idx2, fit1, labels, emb, batch_size, epoch, device, test_frac=None):\n","    model.eval()\n","    criterion = nn.MSELoss()\n","    running_loss = 0.0\n","    total = 0\n","    all_preds = []\n","    all_targets = []\n","    num_batches = math.ceil(len(labels) / batch_size)\n","    b_set = list(range(num_batches))\n","    if test_frac:\n","        b_set = random.sample(range(num_batches), math.ceil(num_batches/test_frac))\n","    with torch.no_grad():\n","        for b in tqdm(b_set, desc=f\"Epoch {epoch}\"):\n","            e1, e2, f1, y = load_data(idx1, idx2, fit1, labels, emb, b, batch_size, device)\n","            preds = model(e1, e2, f1)\n","            loss = criterion(preds, y)\n","            running_loss += loss.item() * preds.size(0)\n","            total += preds.size(0)\n","            all_preds.append(preds.cpu())\n","            all_targets.append(y.cpu())\n","    avg_loss = running_loss / total\n","    all_preds = torch.cat(all_preds).detach().cpu().numpy()\n","    all_targets = torch.cat(all_targets).detach().cpu().numpy()\n","    corr, _ = spearmanr(all_preds, all_targets)\n","    return avg_loss, corr, all_preds, all_targets"],"metadata":{"id":"BJssQQZ5jHIe","executionInfo":{"status":"ok","timestamp":1749540375924,"user_tz":-60,"elapsed":2,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Re-train the CNN_Contrastive model on the entire dataset"],"metadata":{"id":"RJsUXVIam7dw"}},{"cell_type":"code","source":["# Define hyperparameters and instantiate model\n","batch_size    = 16\n","train_frac    = 50                 # fraction of training batches to include in each epoch\n","n_epoch       = train_frac * 2\n","device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = CNN_Contrastive(d_esm=1280, use_first_fitness=True).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=4e-6, weight_decay=1e-4)\n","\n","\n","# --- load & preprocess data ---\n","n_round = 3\n","train_idx1, train_idx2, train_fit1, train_lbl = [], [], [], []\n","\n","emb_list = []\n","offset = 0\n","for r in [f'r{i}' for i in range(1, n_round+1)]:\n","    emb = np.load(f'/content/drive/MyDrive/Mid_1_data_train/ESM_emb_{r}.npy')\n","    emb_list.append(emb)\n","    seq2idx = pickle.load(open(f'/content/drive/MyDrive/Mid_1_data_train/seq_to_index_{r}.pkl','rb'))\n","    df_tr = pd.read_csv(f'/content/drive/MyDrive/Mid_1_data_train/data_contrastive_inference_{r}.csv').sample(frac=1, random_state=1).reset_index(drop=True)\n","\n","    train_idx1 += [seq2idx[s.rstrip('*')]+offset for s in df_tr['seq_1']]\n","    train_idx2 += [seq2idx[s.rstrip('*')]+offset for s in df_tr['seq_2']]\n","    train_fit1 += [float(f) for f in df_tr['fitness_1']]\n","    train_lbl  += [float(y) for y in df_tr['label']]\n","\n","    offset += emb.shape[0]\n","emb = np.concatenate(emb_list)\n","\n","\n","\n","\n","# --- training loop ---\n","\n","train_losses = []\n","\n","for epoch in range(1, n_epoch+1):\n","    tr_loss = train_epoch(model, optimizer,\n","        train_idx1, train_idx2,\n","        train_fit1, train_lbl,\n","        emb, batch_size, epoch, device, train_frac)\n","    train_losses.append(tr_loss)\n","\n","    print(f\"Epoch {epoch:3d} ▶ Train Loss {tr_loss:.4f}\")\n","\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Mid_1_Contrast_results/CNN_Contrastive_inference.pt\")\n","\n","\n","\n","# --- plot loss curve ---\n","epochs = range(len(train_losses))\n","ticks  = np.arange(0, len(train_losses), 10)\n","\n","plt.figure()\n","plt.plot(epochs, train_losses)\n","plt.xticks(ticks)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"MSE Loss\")\n","plt.title(f\"CNN_Contrastive Loss Curve\")\n","plt.savefig(\"/content/drive/MyDrive/Mid_1_Contrast_results/CNN_Contrastive_loss_inference.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-YYfPeXjILd","outputId":"3d500c2e-19a1-4189-b8ec-b6e52b63b382"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 11907/11907 [06:23<00:00, 31.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   1 ▶ Train Loss 8.0359\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 11907/11907 [06:35<00:00, 30.11it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   2 ▶ Train Loss 6.8450\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 11907/11907 [06:18<00:00, 31.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   3 ▶ Train Loss 4.9328\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 11907/11907 [06:45<00:00, 29.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   4 ▶ Train Loss 3.8338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 11907/11907 [06:27<00:00, 30.73it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   5 ▶ Train Loss 2.2549\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 11907/11907 [06:45<00:00, 29.37it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   6 ▶ Train Loss 1.7173\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 11907/11907 [06:33<00:00, 30.22it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   7 ▶ Train Loss 1.4130\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 11907/11907 [06:26<00:00, 30.81it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   8 ▶ Train Loss 1.2198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 11907/11907 [06:24<00:00, 30.93it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch   9 ▶ Train Loss 1.0862\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 11907/11907 [06:21<00:00, 31.17it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  10 ▶ Train Loss 0.9769\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 11: 100%|██████████| 11907/11907 [06:34<00:00, 30.18it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  11 ▶ Train Loss 0.8934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 12: 100%|██████████| 11907/11907 [06:38<00:00, 29.87it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  12 ▶ Train Loss 0.8280\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 13: 100%|██████████| 11907/11907 [06:24<00:00, 30.96it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  13 ▶ Train Loss 0.7691\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 14: 100%|██████████| 11907/11907 [06:20<00:00, 31.26it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  14 ▶ Train Loss 0.7200\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 15: 100%|██████████| 11907/11907 [06:20<00:00, 31.29it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  15 ▶ Train Loss 0.6754\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 16: 100%|██████████| 11907/11907 [06:25<00:00, 30.92it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  16 ▶ Train Loss 0.6361\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 17: 100%|██████████| 11907/11907 [06:16<00:00, 31.63it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  17 ▶ Train Loss 0.6027\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 18: 100%|██████████| 11907/11907 [06:18<00:00, 31.43it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  18 ▶ Train Loss 0.5761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 19: 100%|██████████| 11907/11907 [06:34<00:00, 30.15it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  19 ▶ Train Loss 0.5437\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 20: 100%|██████████| 11907/11907 [06:11<00:00, 32.05it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  20 ▶ Train Loss 0.5170\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 21: 100%|██████████| 11907/11907 [06:31<00:00, 30.42it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  21 ▶ Train Loss 0.4934\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 22: 100%|██████████| 11907/11907 [06:39<00:00, 29.81it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  22 ▶ Train Loss 0.4686\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 23: 100%|██████████| 11907/11907 [06:34<00:00, 30.17it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  23 ▶ Train Loss 0.4534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 24: 100%|██████████| 11907/11907 [06:28<00:00, 30.62it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  24 ▶ Train Loss 0.4324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 25: 100%|██████████| 11907/11907 [06:20<00:00, 31.25it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  25 ▶ Train Loss 0.4140\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 26: 100%|██████████| 11907/11907 [06:20<00:00, 31.33it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  26 ▶ Train Loss 0.3984\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 27: 100%|██████████| 11907/11907 [06:25<00:00, 30.88it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  27 ▶ Train Loss 0.3776\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 28: 100%|██████████| 11907/11907 [06:29<00:00, 30.57it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  28 ▶ Train Loss 0.3590\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 29: 100%|██████████| 11907/11907 [06:27<00:00, 30.75it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  29 ▶ Train Loss 0.3456\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 30: 100%|██████████| 11907/11907 [06:30<00:00, 30.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  30 ▶ Train Loss 0.3305\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 31: 100%|██████████| 11907/11907 [06:26<00:00, 30.81it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  31 ▶ Train Loss 0.3172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 32: 100%|██████████| 11907/11907 [06:27<00:00, 30.71it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  32 ▶ Train Loss 0.3018\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 33: 100%|██████████| 11907/11907 [06:36<00:00, 30.02it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  33 ▶ Train Loss 0.2838\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 34: 100%|██████████| 11907/11907 [06:51<00:00, 28.90it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  34 ▶ Train Loss 0.2746\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 35: 100%|██████████| 11907/11907 [06:29<00:00, 30.56it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  35 ▶ Train Loss 0.2607\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 36: 100%|██████████| 11907/11907 [06:35<00:00, 30.10it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  36 ▶ Train Loss 0.2496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 37: 100%|██████████| 11907/11907 [06:37<00:00, 29.99it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  37 ▶ Train Loss 0.2357\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 38: 100%|██████████| 11907/11907 [06:24<00:00, 30.98it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  38 ▶ Train Loss 0.2259\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 39: 100%|██████████| 11907/11907 [06:27<00:00, 30.72it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  39 ▶ Train Loss 0.2127\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 40: 100%|██████████| 11907/11907 [06:25<00:00, 30.87it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  40 ▶ Train Loss 0.2050\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 41: 100%|██████████| 11907/11907 [06:25<00:00, 30.85it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  41 ▶ Train Loss 0.1921\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 42: 100%|██████████| 11907/11907 [06:30<00:00, 30.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  42 ▶ Train Loss 0.1846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 43: 100%|██████████| 11907/11907 [06:27<00:00, 30.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  43 ▶ Train Loss 0.1745\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 44: 100%|██████████| 11907/11907 [06:31<00:00, 30.45it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  44 ▶ Train Loss 0.1655\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 45: 100%|██████████| 11907/11907 [06:27<00:00, 30.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  45 ▶ Train Loss 0.1605\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 46: 100%|██████████| 11907/11907 [06:32<00:00, 30.30it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  46 ▶ Train Loss 0.1502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 47: 100%|██████████| 11907/11907 [06:39<00:00, 29.78it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  47 ▶ Train Loss 0.1426\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 48: 100%|██████████| 11907/11907 [06:26<00:00, 30.85it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  48 ▶ Train Loss 0.1371\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 49: 100%|██████████| 11907/11907 [06:29<00:00, 30.58it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  49 ▶ Train Loss 0.1314\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 50: 100%|██████████| 11875/11875 [06:20<00:00, 31.17it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  50 ▶ Train Loss 0.1276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 51: 100%|██████████| 11907/11907 [06:24<00:00, 30.95it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  51 ▶ Train Loss 1.2834\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 52: 100%|██████████| 11907/11907 [06:34<00:00, 30.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  52 ▶ Train Loss 1.3179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 53: 100%|██████████| 11907/11907 [06:41<00:00, 29.62it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  53 ▶ Train Loss 0.6289\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 54: 100%|██████████| 11907/11907 [06:25<00:00, 30.91it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  54 ▶ Train Loss 0.4084\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 55: 100%|██████████| 11907/11907 [06:33<00:00, 30.24it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  55 ▶ Train Loss 0.1447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 56: 100%|██████████| 11907/11907 [06:25<00:00, 30.92it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  56 ▶ Train Loss 0.1181\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 57: 100%|██████████| 11907/11907 [06:19<00:00, 31.34it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  57 ▶ Train Loss 0.1076\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 58: 100%|██████████| 11907/11907 [06:38<00:00, 29.85it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  58 ▶ Train Loss 0.1016\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 59: 100%|██████████| 11907/11907 [06:32<00:00, 30.35it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  59 ▶ Train Loss 0.0942\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 60: 100%|██████████| 11907/11907 [06:24<00:00, 30.97it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  60 ▶ Train Loss 0.0899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 61: 100%|██████████| 11907/11907 [06:29<00:00, 30.57it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  61 ▶ Train Loss 0.0878\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 62: 100%|██████████| 11907/11907 [06:30<00:00, 30.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  62 ▶ Train Loss 0.0818\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 63: 100%|██████████| 11907/11907 [06:40<00:00, 29.76it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  63 ▶ Train Loss 0.0784\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 64: 100%|██████████| 11907/11907 [06:24<00:00, 30.98it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  64 ▶ Train Loss 0.0767\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 65: 100%|██████████| 11907/11907 [06:26<00:00, 30.78it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  65 ▶ Train Loss 0.0726\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 66: 100%|██████████| 11907/11907 [06:33<00:00, 30.25it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  66 ▶ Train Loss 0.0703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 67: 100%|██████████| 11907/11907 [06:31<00:00, 30.43it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  67 ▶ Train Loss 0.0688\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 68: 100%|██████████| 11907/11907 [06:22<00:00, 31.11it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  68 ▶ Train Loss 0.0634\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 69: 100%|██████████| 11907/11907 [06:40<00:00, 29.72it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  69 ▶ Train Loss 0.0635\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 70: 100%|██████████| 11907/11907 [06:31<00:00, 30.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  70 ▶ Train Loss 0.0593\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 71: 100%|██████████| 11907/11907 [06:26<00:00, 30.79it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  71 ▶ Train Loss 0.0590\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 72: 100%|██████████| 11907/11907 [06:20<00:00, 31.27it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  72 ▶ Train Loss 0.0573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 73: 100%|██████████| 11907/11907 [06:38<00:00, 29.89it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  73 ▶ Train Loss 0.0571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 74: 100%|██████████| 11907/11907 [06:24<00:00, 31.00it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  74 ▶ Train Loss 0.0534\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 75: 100%|██████████| 11907/11907 [06:26<00:00, 30.78it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  75 ▶ Train Loss 0.0522\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 76: 100%|██████████| 11907/11907 [06:33<00:00, 30.27it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  76 ▶ Train Loss 0.0512\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 77: 100%|██████████| 11907/11907 [06:32<00:00, 30.34it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  77 ▶ Train Loss 0.0484\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 78: 100%|██████████| 11907/11907 [06:40<00:00, 29.74it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  78 ▶ Train Loss 0.0474\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 79: 100%|██████████| 11907/11907 [06:36<00:00, 30.00it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  79 ▶ Train Loss 0.0461\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 80: 100%|██████████| 11907/11907 [06:25<00:00, 30.89it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  80 ▶ Train Loss 0.0476\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 81: 100%|██████████| 11907/11907 [06:22<00:00, 31.10it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  81 ▶ Train Loss 0.0443\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 82: 100%|██████████| 11907/11907 [06:23<00:00, 31.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  82 ▶ Train Loss 0.0433\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 83: 100%|██████████| 11907/11907 [06:27<00:00, 30.72it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  83 ▶ Train Loss 0.0418\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 84: 100%|██████████| 11907/11907 [06:30<00:00, 30.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  84 ▶ Train Loss 0.0436\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 85: 100%|██████████| 11907/11907 [06:36<00:00, 30.03it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  85 ▶ Train Loss 0.0421\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 86: 100%|██████████| 11907/11907 [06:35<00:00, 30.09it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  86 ▶ Train Loss 0.0391\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 87: 100%|██████████| 11907/11907 [06:33<00:00, 30.28it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  87 ▶ Train Loss 0.0383\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 88: 100%|██████████| 11907/11907 [06:22<00:00, 31.16it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  88 ▶ Train Loss 0.0387\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 89: 100%|██████████| 11907/11907 [06:26<00:00, 30.81it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  89 ▶ Train Loss 0.0395\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 90: 100%|██████████| 11907/11907 [06:21<00:00, 31.18it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  90 ▶ Train Loss 0.0362\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 91: 100%|██████████| 11907/11907 [06:27<00:00, 30.74it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  91 ▶ Train Loss 0.0355\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 92: 100%|██████████| 11907/11907 [06:31<00:00, 30.42it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  92 ▶ Train Loss 0.0379\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 93: 100%|██████████| 11907/11907 [06:31<00:00, 30.41it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch  93 ▶ Train Loss 0.0349\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 94:  49%|████▉     | 5819/11907 [03:11<03:19, 30.56it/s]"]}]},{"cell_type":"markdown","source":["# Inference on point mutations"],"metadata":{"id":"i5WuPPL_OuEJ"}},{"cell_type":"code","source":["# 1) Load model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CNN_Contrastive(d_esm=1280, use_first_fitness=True).to(device)\n","checkpoint = \"/content/drive/MyDrive/Mid_1_Contrast_results/CNN_Contrastive_inference.pt\"\n","model.load_state_dict(torch.load(checkpoint, map_location=device))\n","model.eval()\n","\n","# 2) Prepare ESM-2 for embedding\n","esm_model_name = \"facebook/esm2_t33_650M_UR50D\"\n","tokenizer = AutoTokenizer.from_pretrained(esm_model_name, do_lower_case=False)\n","esm = AutoModel.from_pretrained(esm_model_name).to(device)\n","esm.eval()\n","\n","\n","# 3) Build all point‐mutants of the reference seq\n","ref_seq = \"MAGSDSPLAEQIKNTLTFIGQANAAGRMDEVRTLQKNLHPLWAEYFQLTEGSGGSPLAQQIQNGHVLIHQARAAGRMDEVRRLTEKTLQLMKEYFQQSD\"\n","aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n","start_idx = 5    # index start from 1\n","\n","variants = []\n","mutations = []\n","L = len(ref_seq)\n","for i, wt in enumerate(ref_seq):\n","    if i < start_idx-1:\n","      continue\n","    for aa in aas:\n","        if aa == wt:\n","            continue\n","        var = ref_seq[:i] + aa + ref_seq[i+1:]\n","        variants.append(var)\n","        mutations.append(f\"{wt}{i-3}{aa}\")\n","\n","# include the wild‐type itself\n","variants.insert(0, ref_seq)\n","mutations.insert(0, \"\")\n","\n","\n","# 4) Embed mid13sc once (padded/truncated to length 101 tokens)\n","max_len = len(ref_seq) + 2\n","with torch.no_grad():\n","    mid_toks = tokenizer(\n","        ref_seq,\n","        return_tensors=\"pt\",\n","        padding=\"max_length\",\n","        max_length=max_len,\n","        truncation=True,\n","        add_special_tokens=True\n","    ).to(device)\n","    mid_emb = esm(**mid_toks).last_hidden_state.squeeze(0)  # (101,1280)\n","\n","# 5) Batch‐infer all variants\n","batch_size = 32\n","fit1_value = 8.666429599066408\n","results = []\n","\n","for start in tqdm(range(0, len(variants), batch_size), desc=\"Inferring variants\"):\n","    end = min(start + batch_size, len(variants))\n","    batch_seqs = variants[start:end]\n","    batch_muts = mutations[start:end]\n","\n","    # tokenize & embed variants\n","    with torch.no_grad():\n","        toks = tokenizer(\n","            batch_seqs,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            max_length=max_len,\n","            truncation=True,\n","            add_special_tokens=True\n","        ).to(device)\n","        var_emb = esm(**toks).last_hidden_state       # (B, 101, 1280)\n","\n","    # prepare emb1 = repeated mid_emb\n","    B = end - start\n","    emb1 = mid_emb.unsqueeze(0).repeat(B, 1, 1)       # (B, 101, 1280)\n","    emb2 = var_emb                                    # (B, 101, 1280)\n","    fit1 = torch.full((B, 1), fit1_value, device=device)  # (B,1)\n","\n","    # predict\n","    with torch.no_grad():\n","        preds = model(emb1, emb2, fit1)               # (B,)\n","\n","    for seq, mut, score in zip(batch_seqs, batch_muts, preds.cpu().numpy()):\n","        results.append({\n","            \"sequence\": seq,\n","            \"mutation\": mut,\n","            \"predicted_improvement\": score\n","        })\n","\n","# 6) Rank & save\n","df_out = pd.DataFrame(results)\n","df_out = df_out.sort_values(\n","    \"predicted_improvement\",\n","    ascending=False\n",").reset_index(drop=True)\n","\n","out_csv = \"/content/drive/MyDrive/Mid_1_Contrast_results/mid13sc_point_mut_results_cnn.csv\"\n","df_out.to_csv(out_csv, index=False)\n","print(f\"Saved {len(df_out)} variants to {out_csv}\")"],"metadata":{"id":"tMJvUsCCqVHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749542530187,"user_tz":-60,"elapsed":173411,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}},"outputId":"01c66a74-ca73-4240-8c8f-f077b7759104"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Inferring variants: 100%|██████████| 57/57 [02:52<00:00,  3.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved 1806 variants to /content/drive/MyDrive/Mid_1_Contrast_results/mid13sc_point_mut_results_cnn.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# Find where the reference sequence is ranked"],"metadata":{"id":"mDbh5JUVomYc"}},{"cell_type":"code","source":["df_out[df_out['mutation'] == '']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"dXOV9ZxkoulP","executionInfo":{"status":"ok","timestamp":1749542901738,"user_tz":-60,"elapsed":8,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}},"outputId":"715e528b-b4f2-45af-da2e-6848a0ac5f9a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               sequence mutation  \\\n","1120  MAGSDSPLAEQIKNTLTFIGQANAAGRMDEVRTLQKNLHPLWAEYF...            \n","\n","      predicted_improvement  \n","1120              -9.010009  "],"text/html":["\n","  <div id=\"df-a8cf69cb-dac8-41b0-9904-0bdffb2175f9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sequence</th>\n","      <th>mutation</th>\n","      <th>predicted_improvement</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1120</th>\n","      <td>MAGSDSPLAEQIKNTLTFIGQANAAGRMDEVRTLQKNLHPLWAEYF...</td>\n","      <td></td>\n","      <td>-9.010009</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8cf69cb-dac8-41b0-9904-0bdffb2175f9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a8cf69cb-dac8-41b0-9904-0bdffb2175f9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a8cf69cb-dac8-41b0-9904-0bdffb2175f9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","repr_error":"0"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Inference on double/ triple mutations"],"metadata":{"id":"T5qtyYGeoxh1"}},{"cell_type":"code","source":["# 1) Load the previous single‐mutant results and pick top 30 (excluding wildtype)\n","df_single = pd.read_csv(\n","    \"/content/drive/MyDrive/Mid_1_Contrast_results/mid13sc_point_mut_results_cnn.csv\"\n",")\n","# exclude the wild‐type entry which has empty mutation string\n","df_filtered = df_single[df_single['mutation'] != \"\"].reset_index(drop=True)\n","top30 = df_filtered.head(30)\n","mut_list = top30['mutation'].tolist()\n","\n","# 2) Generate all doublet & triplet combinations (only if positions differ)\n","#    parse mutation strings like \"M1D\" → (ref_aa, pos, alt_aa)\n","\n","mut_pattern = re.compile(r'^([A-Z])(\\d+)([A-Z])$')\n","\n","def parse_mut(mut_str):\n","    \"\"\"\n","    Given a mutation string like \"G50A\", parse into:\n","      (0‐based position, new_amino_acid).\n","    According to the new scheme, the integer in the middle is offset by –3,\n","    so actual_index = int(number) + 3.\n","    \"\"\"\n","    m = mut_pattern.match(mut_str)\n","    if m is None:\n","        raise ValueError(f\"Unrecognized mutation format: {mut_str}\")\n","    raw_num = int(m.group(2))\n","    actual_idx = raw_num + 3           # offset by –3 → +3 here\n","    new_aa = m.group(3)\n","    return (actual_idx, new_aa)\n","\n","\n","\n","# Pre‐parse to tuples for fast checks\n","parsed = [parse_mut(m) for m in mut_list]\n","# Keep track of original mutation strings in same order\n","mutation_to_parsed = dict(zip(mut_list, parsed))\n","\n","# Build valid combos\n","doublets = [\n","    combo for combo in itertools.combinations(mut_list, 2)\n","    if mutation_to_parsed[combo[0]][0] != mutation_to_parsed[combo[1]][0]\n","]\n","triplets = [\n","    combo for combo in itertools.combinations(mut_list, 3)\n","    if len({mutation_to_parsed[m][0] for m in combo}) == 3\n","]\n","all_combos = doublets + triplets\n","\n","# 3) Apply mutations to the reference sequence\n","ref_seq = \"MAGSDSPLAEQIKNTLTFIGQANAAGRMDEVRTLQKNLHPLWAEYFQLTEGSGGSPLAQQIQNGHVLIHQARAAGRMDEVRRLTEKTLQLMKEYFQQSD\"\n","\n","\n","def apply_mutations(ref, muts):\n","    arr = list(ref)\n","    for mut in muts:\n","        pos, new_aa = mutation_to_parsed[mut]\n","        arr[pos] = new_aa\n","    return \"\".join(arr)\n","\n","variants   = [apply_mutations(ref_seq, combo) for combo in all_combos]\n","mut_strs   = [\",\".join(combo) for combo in all_combos]\n","\n","\n","# 4) Load model & ESM embedder (assumes you ran the prior snippet)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Re‐load CNN_Contrastive\n","model = CNN_Contrastive(d_esm=1280, use_first_fitness=True).to(device)\n","model.load_state_dict(torch.load(\n","    \"/content/drive/MyDrive/Mid_1_Contrast_results/CNN_Contrastive_inference.pt\",\n","    map_location=device\n","))\n","model.eval()\n","\n","# Re‐load ESM-2\n","from transformers import AutoTokenizer, AutoModel\n","esm_name = \"facebook/esm2_t33_650M_UR50D\"\n","tokenizer = AutoTokenizer.from_pretrained(esm_name, do_lower_case=False)\n","esm = AutoModel.from_pretrained(esm_name).to(device)\n","esm.eval()\n","\n","# Pre‐embed the reference once\n","max_len = len(ref_seq) + 2\n","with torch.no_grad():\n","    tok = tokenizer(\n","        ref_seq,\n","        return_tensors=\"pt\",\n","        padding=\"max_length\",\n","        max_length=max_len,\n","        truncation=True,\n","        add_special_tokens=True\n","    ).to(device)\n","    ref_emb = esm(**tok).last_hidden_state.squeeze(0)  # (101,1280)\n","\n","# 5) Batch‐embed & predict all combos\n","batch_size = 32\n","fit1_value = 8.666429599066408\n","results = []\n","\n","for i in tqdm(range(0, len(variants), batch_size), desc=\"Doublet/Triplet inference\"):\n","    batch_seqs = variants[i:i+batch_size]\n","    batch_muts = mut_strs[i:i+batch_size]\n","    B = len(batch_seqs)\n","\n","    # Tokenize + embed\n","    with torch.no_grad():\n","        toks = tokenizer(\n","            batch_seqs,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            max_length=max_len,\n","            truncation=True,\n","            add_special_tokens=True\n","        ).to(device)\n","        var_emb = esm(**toks).last_hidden_state  # (B,101,1280)\n","\n","    # Prepare emb1, emb2, fit1\n","    emb1 = ref_emb.unsqueeze(0).repeat(B,1,1)         # (B,101,1280)\n","    emb2 = var_emb                                    # (B,101,1280)\n","    fit1 = torch.full((B,1), fit1_value, device=device)\n","\n","    # Predict\n","    with torch.no_grad():\n","        preds = model(emb1, emb2, fit1).cpu().numpy()  # (B,)\n","\n","    # Collect\n","    for seq, muts, score in zip(batch_seqs, batch_muts, preds):\n","        results.append({\n","            \"sequence\": seq,\n","            \"mutations\": muts,\n","            \"predicted_improvement\": score\n","        })\n","\n","# 6) Rank & save\n","df_out = pd.DataFrame(results)\n","df_out = df_out.sort_values(\n","    \"predicted_improvement\",\n","    ascending=False\n",").reset_index(drop=True)\n","\n","out_csv = \"/content/drive/MyDrive/Mid_1_Contrast_results/mid13sc_doublet_triplet_results_cnn.csv\"\n","df_out.to_csv(out_csv, index=False)\n","print(f\"Saved {len(df_out)} combined variants to {out_csv}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-E5Y0xXo1Hs","executionInfo":{"status":"ok","timestamp":1749543295753,"user_tz":-60,"elapsed":390189,"user":{"displayName":"Song Cao","userId":"07583412277585635899"}},"outputId":"e22751bf-b764-4f4f-cfa7-9f0b061c00ec"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Doublet/Triplet inference: 100%|██████████| 128/128 [06:29<00:00,  3.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved 4068 combined variants to /content/drive/MyDrive/Mid_1_Contrast_results/mid13sc_doublet_triplet_results_cnn.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NI4H2wN_wvx4"},"execution_count":null,"outputs":[]}]}